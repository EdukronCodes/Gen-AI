{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Module 12: Advanced Topics\n",
        "\n",
        "### Fine-tuning custom LLMs using LoRA/QLoRA\n",
        "\n",
        "Fine-tuning Large Language Models (LLMs) allows us to adapt their vast general knowledge to specific tasks, domains, or styles. However, full fine-tuning of models with billions of parameters is computationally expensive, requiring significant VRAM and time. LoRA (Low-Rank Adaptation) and its even more memory-efficient variant, QLoRA (Quantized LoRA), offer a parameter-efficient fine-tuning (PEFT) technique. Instead of re-training all the model's weights, LoRA injects small, trainable rank-decomposition matrices (adapters) into the layers of the pre-trained LLM, freezing the original weights. This dramatically reduces the number of trainable parameters, making fine-tuning accessible on consumer-grade hardware while often achieving performance comparable to full fine-tuning.\n",
        "\n",
        "QLoRA takes this a step further by quantizing the pre-trained model to a lower precision (e.g., 4-bit) before adding the LoRA adapters, further reducing memory footprint. This means you can fine-tune even larger models on limited resources. The beauty of these methods is that the original model weights remain untouched, allowing for easy switching between different fine-tuned \"skills\" by simply swapping out the small LoRA adapter weights. This is like having a master toolkit (the base LLM) and adding small, specialized attachments (LoRA adapters) for different jobs, rather than needing a whole new toolkit for each task.\n",
        "\n",
        "**10 Key Points on LoRA/QLoRA:**\n",
        "\n",
        "1.  **Parameter Efficiency:** LoRA significantly reduces the number of trainable parameters compared to full fine-tuning.\n",
        "    This is like customizing a complex machine by only adjusting a few specific dials, not rebuilding the entire engine.\n",
        "2.  **Frozen Base Model:** The original weights of the large pre-trained LLM remain frozen during LoRA training.\n",
        "    This preserves the general knowledge of the base model while specializing it for new tasks.\n",
        "3.  **Adapter Layers:** LoRA injects small, trainable low-rank matrices (adapters) into specific layers (often attention layers) of the LLM.\n",
        "    These adapters learn the task-specific adjustments needed for the new domain or style.\n",
        "4.  **Reduced VRAM:** Due to fewer trainable parameters, LoRA requires much less GPU VRAM for training.\n",
        "    This makes fine-tuning accessible on consumer GPUs, democratizing LLM customization.\n",
        "5.  **Faster Training:** Training LoRA adapters is typically faster than full fine-tuning.\n",
        "    Less data needs to be backpropagated through fewer parameters, speeding up each epoch.\n",
        "6.  **QLoRA for Ultra-Efficiency:** QLoRA combines LoRA with quantization of the base model (e.g., to 4-bit).\n",
        "    This drastically cuts memory usage further, allowing even larger models to be fine-tuned on single GPUs.\n",
        "7.  **No Catastrophic Forgetting:** Since base weights are frozen, the model is less prone to \"catastrophic forgetting\" of its original capabilities.\n",
        "    The core knowledge is preserved, and new skills are layered on top via the adapters.\n",
        "8.  **Easy Task Switching:** Multiple LoRA adapters can be trained for different tasks and easily swapped out.\n",
        "    This is like having different \"personality chips\" or \"skill modules\" for the same base AI.\n",
        "9.  **Comparable Performance:** Despite its efficiency, LoRA/QLoRA often achieves performance close to full fine-tuning on many tasks.\n",
        "    It's a highly effective trade-off between resource cost and model quality for specialization.\n",
        "10. **Integration with Hugging Face:** Libraries like Hugging Face's `PEFT` (Parameter-Efficient Fine-Tuning) provide easy-to-use implementations of LoRA and QLoRA.\n",
        "    This simplifies the process of applying these advanced techniques to various open-source LLMs.\n",
        "\n",
        "---\n",
        "\n",
        "### RAG + Agents (Hybrid chains)\n",
        "\n",
        "Retrieval Augmented Generation (RAG) enhances LLMs by grounding their responses in external, up-to-date knowledge bases. Instead of relying solely on its training data (which can be outdated or lack specific private information), a RAG system first retrieves relevant documents from a vector store or other database based on the user's query. These retrieved documents are then provided as context to the LLM, which uses them to generate a more informed and accurate answer. This mitigates hallucinations and allows LLMs to \"know\" things beyond their initial training.\n",
        "\n",
        "When RAG is combined with Agents, we create powerful \"hybrid chains.\" Agents are LLM-powered systems that can use tools (like search engines, calculators, APIs, or even other chains) and make decisions to accomplish complex tasks. A RAG-powered Agent can thus retrieve information (using RAG as a tool) and then reason about that information, potentially using other tools, to achieve a multi-step goal. For instance, an agent might first use RAG to understand current market trends for a product, then use a calculator tool to estimate potential profits, and finally use an email tool to draft a summary for a stakeholder. This hybrid approach allows for more robust, factual, and capable AI systems that can interact with the world and access dynamic information.\n",
        "\n",
        "**10 Key Points on RAG + Agents:**\n",
        "\n",
        "1.  **RAG for Factual Grounding:** RAG systems retrieve relevant information from external knowledge bases to provide context for LLM generation.\n",
        "    This is like giving a student an open book for an exam, ensuring their answers are based on provided facts.\n",
        "2.  **Agents for Autonomous Action:** Agents use an LLM as a reasoning engine to decide which actions to take, often involving tools.\n",
        "    Think of an agent as a project manager that can delegate tasks to different tools (including RAG) to achieve a goal.\n",
        "3.  **Hybrid Chains Combine Strengths:** Combining RAG with Agents allows the agent to dynamically fetch knowledge and then act upon it.\n",
        "    This creates systems that are both knowledgeable (via RAG) and capable of complex task execution (via Agentic reasoning).\n",
        "4.  **Reduced Hallucinations:** By grounding responses in retrieved data, RAG components significantly reduce the LLM's tendency to invent facts.\n",
        "    The agent's decisions are then based on more reliable, contextually relevant information.\n",
        "5.  **Access to Real-time Data:** RAG can connect to dynamic data sources, allowing the Agent to operate with up-to-date information.\n",
        "    An agent planning a trip can use RAG to get current flight prices or weather forecasts.\n",
        "6.  **Tool Use for Agents:** RAG can be one of many \"tools\" an agent has. Other tools might include code interpreters, search engines, or database query tools.\n",
        "    The agent decides when and how to use RAG based on the task at hand, like a chef choosing the right knife.\n",
        "7.  **Multi-step Reasoning:** Agents excel at breaking down complex problems into smaller, manageable steps, using tools (including RAG) at each step.\n",
        "    For instance, \"Summarize recent news about AI and then draft a tweet\" involves RAG for news, then an LLM call for summarization and tweet drafting.\n",
        "8.  **Improved Explainability:** Because RAG provides sources, the agent's information-gathering step becomes more transparent.\n",
        "    Users can often see which documents were retrieved and used, increasing trust in the agent's outputs.\n",
        "9.  **Domain Specialization:** The knowledge base used by RAG can be domain-specific (e.g., medical journals, legal documents, company wikis).\n",
        "    This allows an agent to become an \"expert\" in a particular field by having access to specialized data.\n",
        "10. **Iterative Refinement:** An agent can iteratively use RAG, refining its query based on initial results to dig deeper or broaden its search.\n",
        "    This mimics human research, where initial findings guide further investigation for a more comprehensive understanding.\n",
        "\n",
        "---\n",
        "\n",
        "## Module 9: Evaluation and Testing\n",
        "\n",
        "### Multi-modal models (Text + Image)\n",
        "\n",
        "Multi-modal models are AI systems designed to process and understand information from multiple types of data, most commonly text and images. Unlike traditional models that might specialize in only natural language processing or only computer vision, multi-modal models can find relationships, generate content, or answer questions based on combined inputs. For example, a multi-modal model could describe an image in text (image captioning), generate an image based on a textual description (text-to-image generation), or answer questions about an image (Visual Question Answering - VQA).\n",
        "\n",
        "Evaluating these models is more complex than evaluating uni-modal systems because it requires assessing performance across different modalities and, crucially, the model's ability to correlate them. Metrics often combine traditional NLP scores (like BLEU or ROUGE for generated text) with vision-related scores or new metrics designed specifically for multi-modal tasks, such as CLIPScore, which measures the semantic similarity between an image and a text description. Human evaluation also plays a critical role, as automated metrics may not fully capture the nuances of image-text coherence, relevance, or visual quality.\n",
        "\n",
        "**10 Key Points on Evaluating Multi-modal (Text + Image) Models:**\n",
        "\n",
        "1.  **Task-Specific Metrics:** Evaluation depends heavily on the specific multi-modal task (e.g., VQA, image captioning, text-to-image).\n",
        "    For VQA, accuracy is key; for captioning, fluency and relevance (e.g., CIDEr, SPICE) are important.\n",
        "2.  **Image Captioning Evaluation:** Metrics like BLEU, ROUGE, METEOR, and CIDEr compare generated captions against human-written reference captions.\n",
        "    These assess n-gram overlap, synonymy, and consensus among references.\n",
        "3.  **Visual Question Answering (VQA) Evaluation:** Typically involves accuracy, where the model's answer to a question about an image is compared to a ground truth answer.\n",
        "    Specialized datasets like VQA v2 are used, often requiring simple, factual answers.\n",
        "4.  **Text-to-Image Generation Evaluation:** Metrics like Fr√©chet Inception Distance (FID) and Inception Score (IS) assess the quality and diversity of generated images.\n",
        "    These compare statistical properties of generated images to real images but don't directly measure text alignment.\n",
        "5.  **CLIPScore for Image-Text Alignment:** CLIPScore uses a pre-trained CLIP model to measure the semantic similarity between a generated image and its input text prompt, or an image and its generated caption.\n",
        "    This directly assesses how well the image and text correspond to each other conceptually.\n",
        "6.  **Human Evaluation is Crucial:** Automated metrics often fail to capture nuances like aesthetic quality, common sense, or subtle misalignments.\n",
        "    Human reviewers are essential for judging overall quality, relevance, and coherence in multi-modal outputs.\n",
        "7.  **Compositionality Testing:** Assessing if the model understands combinations of objects, attributes, and relations described in text and depicted in images.\n",
        "    For example, can it distinguish \"a red cube on a blue sphere\" from \"a blue cube on a red sphere\"?\n",
        "8.  **Robustness and Adversarial Testing:** Evaluating how models perform with noisy inputs, slight variations, or adversarial attacks in either modality.\n",
        "    This tests the model's stability and reliability in real-world, imperfect conditions.\n",
        "9.  **Bias and Fairness Evaluation:** Multi-modal models can inherit and amplify biases present in their training data (e.g., stereotypical depictions).\n",
        "    Evaluation must include checks for demographic biases in image generation or captioning.\n",
        "10. **Zero-shot/Few-shot Performance:** Assessing the model's ability to perform tasks or understand concepts it hasn't explicitly been trained on, using its cross-modal understanding.\n",
        "    This tests the generalization capabilities learned from large-scale multi-modal pre-training.\n",
        "\n",
        "---\n",
        "\n",
        "### LangGraph (workflow orchestration for LangChain)\n",
        "\n",
        "LangGraph is an extension of the LangChain library designed to build robust, stateful, and potentially cyclical agentic applications or multi-step chains as graphs. While LangChain provides excellent tools for creating linear sequences of LLM calls and tool usage (chains), complex applications often require more intricate control flow, including loops, conditional branching, and persistent state across multiple interactions. LangGraph addresses this by allowing developers to define their workflows as a graph of nodes and edges, where nodes represent functions or LLM calls (actions) and edges represent the transitions between them based on specific conditions.\n",
        "\n",
        "This graph-based approach provides a more explicit and manageable way to construct sophisticated agent behaviors. Each node in the graph can modify a shared \"state\" object, allowing information to be passed and updated throughout the execution of the graph. This is particularly useful for building agents that need to iterate, reflect, or dynamically plan their next steps, mimicking more complex human-like reasoning processes. It's like creating a detailed flowchart for your AI, where it can loop back, make decisions, and maintain context over extended interactions.\n",
        "\n",
        "**10 Key Points on LangGraph:**\n",
        "\n",
        "1.  **Graph-Based Workflows:** LangGraph allows you to define LLM applications as directed graphs with nodes and edges.\n",
        "    Nodes represent computational steps (LLM calls, tool use), and edges define the flow between these steps.\n",
        "2.  **State Management:** A central \"state\" object is passed between nodes, allowing for persistent information across the graph's execution.\n",
        "    This is crucial for tasks requiring memory or context accumulation over multiple steps, like a conversation history.\n",
        "3.  **Cyclical Processes:** Unlike many standard LangChain chains, LangGraph explicitly supports cycles and loops in the workflow.\n",
        "    This enables iterative refinement, self-correction, or retrying actions until a condition is met.\n",
        "4.  **Conditional Edges:** Transitions between nodes (edges) can be conditional, allowing for dynamic routing based on the current state or output of a node.\n",
        "    This facilitates complex decision-making within the agent, like choosing different tools based on query type.\n",
        "5.  **Agentic Architectures:** LangGraph is well-suited for building complex agent architectures like ReAct (Reasoning and Acting) or reflection agents.\n",
        "    It provides the control flow needed for an agent to plan, execute, observe, and re-plan.\n",
        "6.  **Modularity and Reusability:** Nodes in the graph can be modular functions, making the overall workflow easier to understand, debug, and modify.\n",
        "    Individual components of the agent's logic can be developed and tested independently.\n",
        "7.  **Explicit Control Flow:** Developers have explicit control over the sequence of operations and the conditions for transitions.\n",
        "    This contrasts with some implicit agent loops in core LangChain, offering more fine-grained orchestration.\n",
        "8.  **Human-in-the-Loop Integration:** The graph structure can easily accommodate points where human input or approval is required before proceeding.\n",
        "    A node can be designed to pause execution and await external validation or guidance.\n",
        "9.  **Visualization and Debugging:** Graph structures are inherently visual, which can aid in understanding the agent's logic and debugging its execution flow.\n",
        "    Though visualization tools might be external, the conceptual model is clear.\n",
        "10. **Building Multi-Agent Systems:** LangGraph can be used to orchestrate interactions between multiple specialized agents, each represented as a node or subgraph.\n",
        "    This enables complex collaborative problem-solving among different AI components.\n",
        "\n",
        "---\n",
        "\n",
        "### Caching strategies (LangChain cache, Redis)\n",
        "\n",
        "Caching in the context of LLM applications involves storing the results of expensive or frequently repeated operations, primarily LLM calls, to avoid redundant computation and API costs. When a request is made that has been processed before (e.g., the same prompt sent to an LLM), the cached result can be returned instantly, improving speed and reducing expenses. LangChain provides built-in caching mechanisms, and it can also integrate with external caching solutions like Redis for more persistent and scalable caching.\n",
        "\n",
        "LangChain's default in-memory cache is simple and useful for single sessions, storing prompt-completion pairs. For applications that run across multiple sessions or require more robust caching, an external key-value store like Redis is highly effective. Redis offers persistence (data survives application restarts), shared access (multiple application instances can use the same cache), and advanced features like configurable eviction policies (e.g., LRU - Least Recently Used) to manage cache size. Implementing an effective caching strategy is crucial for optimizing performance and cost in production LLM systems, especially when dealing with repetitive queries or common sub-tasks.\n",
        "\n",
        "**10 Key Points on Caching Strategies:**\n",
        "\n",
        "1.  **Purpose of Caching:** To store and reuse results of LLM calls or other expensive computations to save time and cost.\n",
        "    It's like a chef pre-chopping common ingredients to speed up meal preparation during peak hours.\n",
        "2.  **LangChain In-Memory Cache:** LangChain offers a simple default in-memory cache (`InMemoryCache`).\n",
        "    This is useful for development or single-session applications but is not persistent across restarts.\n",
        "3.  **SQLite Cache:** LangChain also supports `SQLiteCache`, which stores cache entries in a local SQLite database file.\n",
        "    This provides persistence on a single machine, surviving application restarts.\n",
        "4.  **External Caching with Redis:** Redis is a popular in-memory data store often used as a distributed, persistent cache.\n",
        "    LangChain can integrate with Redis (`RedisCache`) for robust, shared caching across multiple application instances or services.\n",
        "5.  **Cache Keys:** Caching systems use a \"key\" (often a hash of the LLM prompt and model parameters) to store and retrieve results.\n",
        "    If the exact same request (key) comes in, the stored (cached) response is returned.\n",
        "6.  **Reduced Latency:** Retrieving a result from a cache (especially an in-memory one like Redis) is significantly faster than making a new LLM API call.\n",
        "    This leads to a much snappier user experience for repeated queries.\n",
        "7.  **Cost Savings:** For pay-per-use LLM APIs, caching identical requests directly translates to reduced operational costs.\n",
        "    Fewer API calls mean lower bills from the LLM provider.\n",
        "8.  **Cache Eviction Policies:** When a cache reaches its size limit, an eviction policy (e.g., LRU - Least Recently Used, LFU - Least Frequently Used) decides which items to remove.\n",
        "    This ensures the cache doesn't grow indefinitely and prioritizes keeping more relevant items.\n",
        "9.  **Cache Invalidation:** A challenge in caching is knowing when cached data becomes stale and needs to be updated or removed.\n",
        "    For LLMs, this is less of an issue if prompts are identical, but crucial if underlying data sources change.\n",
        "10. **Semantic Caching (Advanced):** Beyond exact-match caching, semantic caching aims to cache results for semantically similar prompts.\n",
        "    This is more complex but can provide even greater efficiency by recognizing paraphrased or conceptually identical queries.\n",
        "\n",
        "---\n",
        "\n",
        "### Streaming output and callbacks\n",
        "\n",
        "Streaming output in LLM applications refers to the practice of sending the LLM's response to the user token by token (or word by word) as it's being generated, rather than waiting for the entire response to be complete. This significantly improves the perceived performance and user experience, as users start seeing results almost immediately, much like how ChatGPT types out its answers. It keeps the user engaged and provides a sense of responsiveness, especially for longer generations.\n",
        "\n",
        "Callbacks in LangChain are functions that get triggered at various points during the execution of a chain or an LLM call. They provide a powerful mechanism for logging, monitoring, debugging, or instrumenting the behavior of your LLM application. For instance, a callback can log every prompt sent to an LLM, track token usage, record errors, or even trigger custom actions when a chain starts or finishes. Streaming itself is often implemented using a specific type of callback (`StreamingStdOutCallbackHandler` or custom handlers) that handles each newly generated token.\n",
        "\n",
        "**10 Key Points on Streaming Output and Callbacks:**\n",
        "\n",
        "1.  **Improved User Experience (Streaming):** Streaming output displays text as it's generated by the LLM, token by token.\n",
        "    This makes the application feel much faster and more interactive, like a live conversation.\n",
        "2.  **LangChain Streaming Support:** Many LangChain LLM wrappers and chains support a `stream()` method or can be configured for streaming.\n",
        "    This typically yields an iterator or uses a callback mechanism to provide tokens as they arrive.\n",
        "3.  **Callbacks for Event Handling:** Callbacks are functions that LangChain executes at specific lifecycle events of chains or LLM calls.\n",
        "    Events include `on_llm_start`, `on_llm_new_token`, `on_chain_end`, `on_tool_error`, etc.\n",
        "4.  **`CallbackManager` and `CallbackHandler`:** LangChain uses a `CallbackManager` to orchestrate multiple `CallbackHandler`s.\n",
        "    Handlers implement methods for different events; `StreamingStdOutCallbackHandler` is a common one for printing tokens to console.\n",
        "5.  **Real-time Feedback:** Streaming provides immediate feedback, assuring the user that the system is working, especially for long generations.\n",
        "    It avoids the \"is it stuck?\" feeling that can occur when waiting for a large block of text.\n",
        "6.  **Detailed Logging and Monitoring:** Callbacks are invaluable for logging prompts, responses, errors, and intermediate steps.\n",
        "    This aids in debugging, performance analysis, and understanding agent behavior.\n",
        "7.  **Token Usage Tracking:** Callbacks like `on_llm_end` often receive information about token usage for the call.\n",
        "    This is essential for monitoring costs and managing API rate limits.\n",
        "8.  **Custom Callback Handlers:** Developers can create custom callback handlers to perform specific actions, like sending data to a logging service or updating a UI.\n",
        "    This provides great flexibility for integrating LangChain operations with external systems.\n",
        "9.  **Debugging Agent Behavior:** Callbacks can print the thoughts and actions of an agent as it works, providing insight into its decision-making process.\n",
        "    For instance, `on_agent_action` and `on_agent_finish` are very useful for this.\n",
        "10. **Combining Streaming with Callbacks:** Streaming is often implemented via a callback handler (e.g., `AsyncIteratorCallbackHandler`) that specifically processes `on_llm_new_token` events to yield tokens.\n",
        "    This neatly integrates the real-time output mechanism into LangChain's event-driven architecture."
      ],
      "metadata": {
        "id": "0DgxKcuxVsU0"
      }
    }
  ]
}