{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qq5wgUWpkqa1"
      },
      "source": [
        "# \"Text Preprocessing\"\n",
        "> \"Vairous preprocessing steps required to clean and prepare your data in NLP\"\n",
        "\n",
        "- toc: true\n",
        "- branch: master\n",
        "- badges: true\n",
        "- comments: true\n",
        "- categories: [Text Preprocessing, Natural Language Processing]\n",
        "- image: images/posts/text_preprocessing.jpg\n",
        "- hide: false\n",
        "- search_exclude: false"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4OriOYHkqa3"
      },
      "source": [
        "In any machine learning task, cleaning and pre-processing of the data is a very important step. The better we can represent our data, the better the model training and prediction can be expected.\n",
        "\n",
        "Specially in the domain of Natural Language Processing (NLP) the data is unstructured. It become crucial to clean and properly format it based on the task at hand. There are various pre-processing steps that can be performed but not necessary to perform all. These steps should be applied based on the problem statement.\n",
        "\n",
        "Example: Sentiment analysis on twitter data can required to remove hashtags, emoticons, etc. but this may not be the case if we are doing the same analysis on customer feedback data.\n",
        "\n",
        "Here we are using the twitter_sample dataset from the nltk library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gh-_-_Tukqa4",
        "outputId": "dfd78580-58d7-4b58-8571-aae27f3ca1e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'demoji'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-44a840dbb66f>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdemoji\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontractions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munidecode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'demoji'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "#collapse-hide\n",
        "# Import libraries and load the data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import string\n",
        "import demoji\n",
        "import contractions\n",
        "import unidecode\n",
        "from num2words import num2words\n",
        "from nltk.corpus import twitter_samples\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from bs4 import BeautifulSoup\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "pd.options.display.max_columns=None\n",
        "pd.options.display.max_rows=None\n",
        "pd.options.display.max_colwidth=None\n",
        "\n",
        "nltk.download('twitter_samples') # Download the dataset\n",
        "\n",
        "# We are going to use the Negative and Positive Tweets file which each contains 5000 tweets.\n",
        "for name in twitter_samples.fileids():\n",
        "    print(f' - {name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6aVWKTgkqa5"
      },
      "outputs": [],
      "source": [
        "#collapse-hide\n",
        "\n",
        "# Load the negative tweets file and assign label as 0 for negative\n",
        "negative_tweets = twitter_samples.strings(\"negative_tweets.json\")\n",
        "df_neg = pd.DataFrame(negative_tweets, columns=['text'])\n",
        "df_neg['label'] = 0\n",
        "\n",
        "# Load the positive tweets file and assign label as 1 for positive\n",
        "positive_tweets = twitter_samples.strings(\"positive_tweets.json\")\n",
        "df_pos = pd.DataFrame(positive_tweets, columns=['text'])\n",
        "df_pos['label'] = 1\n",
        "\n",
        "df = pd.concat([df_pos, df_neg]) # Concatenate both the files\n",
        "df = df.sample(frac=1).reset_index(drop=True) # Shuffle the data to mix negative and positive tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7m8SWRYkqa6"
      },
      "outputs": [],
      "source": [
        "print(f'Shape of the whole data is: {df.shape[0]} rows and {df.shape[1]} columns')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47vK6o6_kqa6"
      },
      "outputs": [],
      "source": [
        "# Look at the head of the dataframe\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh6ehTQZkqa6"
      },
      "source": [
        "> Note: Always make it a practice to first skim the dataset before performing any text pre-processing steps. It is important because text data can be very noisy eg. dates are written in different formats, present of accented characters, etc. These are stuff we can easily miss if we don't go through the dataset properly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euMsNeGekqa6"
      },
      "source": [
        "## Lower Casing\n",
        "\n",
        "Lowercasing is a common text preprocessing technique. It helps to transform all the text in same case. <br>\n",
        "Examples 'The', 'the', 'ThE' -> 'the'\n",
        "\n",
        "This is also useful to find all the duplicates since words in different cases are treated as separate words and becomes difficult for us to remove redundant words in all different case combination.\n",
        "\n",
        "This may not be helpful when we do tasks like Part of Speech tagging (where proper casing gives some information about Nouns and so on) and Sentiment Analysis (where upper casing refers to anger and so on)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqfzZm-vkqa6"
      },
      "outputs": [],
      "source": [
        "df.text = df.text.str.lower()\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szcIJfdjkqa7"
      },
      "source": [
        "## Remove"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouVLgX57kqa7"
      },
      "source": [
        "### URL's"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLGhDSLXkqa7"
      },
      "source": [
        "URL stands for Uniform Resource Locator. If present in a text, it represents the location of another website.\n",
        "\n",
        "If we are performing any websites backlink analysis, in that case URL's are useful to keep. Otherwise, they don't provide any information. So we can remove them from our text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hRzEaLPkqa8"
      },
      "outputs": [],
      "source": [
        "df.text = df.text.str.replace(r'https?://\\S+|www\\.\\S+', '', regex=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqWZbTrnkqa8"
      },
      "source": [
        "### E-mail"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG70BYk0kqa8"
      },
      "source": [
        "E-mail id's are common in customer feedback data and they do not provide any useful information. So we remove them from the text.\n",
        "\n",
        "Twitter data that we are using does not contain any email id's. Hence, please find the code snipper with an dummy example to remove e-mail id's."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNVj1sJrkqa8"
      },
      "outputs": [],
      "source": [
        "text = 'I have being trying to contact xyz via email to xyz@abc.co.in but there is no response.'\n",
        "re.sub(r'\\S+@\\S+', '', text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhbJD9U7kqa8"
      },
      "source": [
        "### Date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5Du-kbLkqa9"
      },
      "source": [
        "Dates can be represented in various formats and can be difficult at times to remove them. They are unlikely to contain any useful information for predicting the labels.\n",
        "\n",
        "Below I have used dummy text to showcase the following task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-TDJ3xrkqa9"
      },
      "outputs": [],
      "source": [
        "text = \"Today is 22/12/2020 and after two days on 24-12-2020 our vacation starts until 25th.09.2021\"\n",
        "\n",
        "# 1. Remove date formats like: dd/mm/yy(yy), dd-mm-yy(yy), dd(st|nd|rd).mm/yy(yy)\n",
        "re.sub(r'\\d{1,2}(st|nd|rd|th)?[-./]\\d{1,2}[-./]\\d{2,4}', '', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqTRs9LRkqa9"
      },
      "outputs": [],
      "source": [
        "text = \"Today is 11th of January, 2021 when I am writing this post. I hope to post this by February 15th or max to max by 20 may 21 or 20th-December-21\"\n",
        "\n",
        "# 2. Remove date formats like: 20 apr 21, April 15th, 11th of April, 2021\n",
        "pattern = re.compile(r'(\\d{1,2})?(st|nd|rd|th)?[-./,]?\\s?(of)?\\s?([J|j]an(uary)?|[F|f]eb(ruary)?|[Mm]ar(ch)?|[Aa]pr(il)?|[Mm]ay|[Jj]un(e)?|[Jj]ul(y)?|[Aa]ug(ust)?|[Ss]ep(tember)?|[Oo]ct(ober)?|[Nn]ov(ember)?|[Dd]ec(ember)?)\\s?(\\d{1,2})?(st|nd|rd|th)?\\s?[-./,]?\\s?(\\d{2,4})?')\n",
        "pattern.sub(r'', text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1brkqTTbkqa9"
      },
      "source": [
        "There are various formats in which dates are represented and the above regex can be customized in many ways. Above, \"byor\" got combined cause we are trying multiple format in single regex pattern. You can customize the above expression accordingly to your need."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-m_LWs1kqa9"
      },
      "source": [
        "### HTML Tags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y04N5pD3kqa-"
      },
      "source": [
        "If we are extracting data from various websites, it is possible that the data also contains HTML tags. These tags does not provide any information and should be removed. These tags can be removed using regex or by using BeautifulSoup library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6QgX0Mkkqa-"
      },
      "outputs": [],
      "source": [
        "# Dummy text\n",
        "text = \"\"\"\n",
        "<title>Below is a dummy html code.</title>\n",
        "<body>\n",
        "    <p>All the html opening and closing brackets should be remove.</p>\n",
        "    <a href=\"https://www.abc.com\">Company Site</a>\n",
        "</body>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBJo-BHDkqa-"
      },
      "outputs": [],
      "source": [
        "# Using regex to remove html tags\n",
        "pattern = re.compile('<.*?>')\n",
        "pattern.sub('', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AU-04bWPkqa-"
      },
      "outputs": [],
      "source": [
        "# Using Beautiful Soup\n",
        "def remove_html(text):\n",
        "    clean_text = BeautifulSoup(text).get_text()\n",
        "    return clean_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFM6Qr9xkqa-"
      },
      "outputs": [],
      "source": [
        "remove_html(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dkrbPXWkqa_"
      },
      "source": [
        "### Emojis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCOgh2iMkqa_"
      },
      "source": [
        "As more and more people have started using social media emoji's play a very crucial role. Emoji's are used to express emotions that are universally understood.\n",
        "\n",
        "In some analysis such as sentiment analysis emoji's can be useful. We can convert them to words or create some new features based on them. For some analysis we need to remove them. Find the below code snippet used to remove the emoji's."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tr1_Jxc1kqa_"
      },
      "outputs": [],
      "source": [
        "#collapse-hide\n",
        "# Reference: https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
        "\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEk5-BL1kqa_"
      },
      "outputs": [],
      "source": [
        "text = \"game is on üî•üî•. HilariousüòÇ\"\n",
        "remove_emoji(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tpT5G-5kqbA"
      },
      "outputs": [],
      "source": [
        "# Remove emoji's from text\n",
        "df.text = df.text.apply(lambda x: remove_emoji(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgHbi55skqbA"
      },
      "source": [
        "### Emoticons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlOdzZ31kqbA"
      },
      "source": [
        "Emoji's and Emoticons are different. Yes!!<br>\n",
        "Emoticons are used to express facial expressions using keyboard characters such as letters, numbers, and pucntuation marks. Where emjoi's are small images.\n",
        "\n",
        "Thanks to [Neel Shah](https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py) for curating a dictionary of emoticons and their description. We shall use this dictionary and remove the emoticons from our text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuwHJoorkqbB"
      },
      "outputs": [],
      "source": [
        "#collapse-hide\n",
        "\n",
        "EMOTICONS = {\n",
        "    u\":‚Äë\\)\":\"Happy face or smiley\",\n",
        "    u\":\\)\":\"Happy face or smiley\",\n",
        "    u\":-\\]\":\"Happy face or smiley\",\n",
        "    u\":\\]\":\"Happy face or smiley\",\n",
        "    u\":-3\":\"Happy face smiley\",\n",
        "    u\":3\":\"Happy face smiley\",\n",
        "    u\":->\":\"Happy face smiley\",\n",
        "    u\":>\":\"Happy face smiley\",\n",
        "    u\"8-\\)\":\"Happy face smiley\",\n",
        "    u\":o\\)\":\"Happy face smiley\",\n",
        "    u\":-\\}\":\"Happy face smiley\",\n",
        "    u\":\\}\":\"Happy face smiley\",\n",
        "    u\":-\\)\":\"Happy face smiley\",\n",
        "    u\":c\\)\":\"Happy face smiley\",\n",
        "    u\":\\^\\)\":\"Happy face smiley\",\n",
        "    u\"=\\]\":\"Happy face smiley\",\n",
        "    u\"=\\)\":\"Happy face smiley\",\n",
        "    u\":‚ÄëD\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\":D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"8‚ÄëD\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"8D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"X‚ÄëD\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"XD\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"=D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"=3\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"B\\^D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\":-\\)\\)\":\"Very happy\",\n",
        "    u\":‚Äë\\(\":\"Frown, sad, andry or pouting\",\n",
        "    u\":-\\(\":\"Frown, sad, andry or pouting\",\n",
        "    u\":\\(\":\"Frown, sad, andry or pouting\",\n",
        "    u\":‚Äëc\":\"Frown, sad, andry or pouting\",\n",
        "    u\":c\":\"Frown, sad, andry or pouting\",\n",
        "    u\":‚Äë<\":\"Frown, sad, andry or pouting\",\n",
        "    u\":<\":\"Frown, sad, andry or pouting\",\n",
        "    u\":‚Äë\\[\":\"Frown, sad, andry or pouting\",\n",
        "    u\":\\[\":\"Frown, sad, andry or pouting\",\n",
        "    u\":-\\|\\|\":\"Frown, sad, andry or pouting\",\n",
        "    u\">:\\[\":\"Frown, sad, andry or pouting\",\n",
        "    u\":\\{\":\"Frown, sad, andry or pouting\",\n",
        "    u\":@\":\"Frown, sad, andry or pouting\",\n",
        "    u\">:\\(\":\"Frown, sad, andry or pouting\",\n",
        "    u\":'‚Äë\\(\":\"Crying\",\n",
        "    u\":'\\(\":\"Crying\",\n",
        "    u\":'‚Äë\\)\":\"Tears of happiness\",\n",
        "    u\":'\\)\":\"Tears of happiness\",\n",
        "    u\"D‚Äë':\":\"Horror\",\n",
        "    u\"D:<\":\"Disgust\",\n",
        "    u\"D:\":\"Sadness\",\n",
        "    u\"D8\":\"Great dismay\",\n",
        "    u\"D;\":\"Great dismay\",\n",
        "    u\"D=\":\"Great dismay\",\n",
        "    u\"DX\":\"Great dismay\",\n",
        "    u\":‚ÄëO\":\"Surprise\",\n",
        "    u\":O\":\"Surprise\",\n",
        "    u\":‚Äëo\":\"Surprise\",\n",
        "    u\":o\":\"Surprise\",\n",
        "    u\":-0\":\"Shock\",\n",
        "    u\"8‚Äë0\":\"Yawn\",\n",
        "    u\">:O\":\"Yawn\",\n",
        "    u\":-\\*\":\"Kiss\",\n",
        "    u\":\\*\":\"Kiss\",\n",
        "    u\":X\":\"Kiss\",\n",
        "    u\";‚Äë\\)\":\"Wink or smirk\",\n",
        "    u\";\\)\":\"Wink or smirk\",\n",
        "    u\"\\*-\\)\":\"Wink or smirk\",\n",
        "    u\"\\*\\)\":\"Wink or smirk\",\n",
        "    u\";‚Äë\\]\":\"Wink or smirk\",\n",
        "    u\";\\]\":\"Wink or smirk\",\n",
        "    u\";\\^\\)\":\"Wink or smirk\",\n",
        "    u\":‚Äë,\":\"Wink or smirk\",\n",
        "    u\";D\":\"Wink or smirk\",\n",
        "    u\":‚ÄëP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"X‚ÄëP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"XP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":‚Äë√û\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":√û\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"d:\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"=p\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\">:P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":‚Äë/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":-[.]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\">:[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\">:/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\"=/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\"=[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\"=L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":S\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":‚Äë\\|\":\"Straight face\",\n",
        "    u\":\\|\":\"Straight face\",\n",
        "    u\":$\":\"Embarrassed or blushing\",\n",
        "    u\":‚Äëx\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":‚Äë#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":‚Äë&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\"O:‚Äë\\)\":\"Angel, saint or innocent\",\n",
        "    u\"O:\\)\":\"Angel, saint or innocent\",\n",
        "    u\"0:‚Äë3\":\"Angel, saint or innocent\",\n",
        "    u\"0:3\":\"Angel, saint or innocent\",\n",
        "    u\"0:‚Äë\\)\":\"Angel, saint or innocent\",\n",
        "    u\"0:\\)\":\"Angel, saint or innocent\",\n",
        "    u\":‚Äëb\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"0;\\^\\)\":\"Angel, saint or innocent\",\n",
        "    u\">:‚Äë\\)\":\"Evil or devilish\",\n",
        "    u\">:\\)\":\"Evil or devilish\",\n",
        "    u\"\\}:‚Äë\\)\":\"Evil or devilish\",\n",
        "    u\"\\}:\\)\":\"Evil or devilish\",\n",
        "    u\"3:‚Äë\\)\":\"Evil or devilish\",\n",
        "    u\"3:\\)\":\"Evil or devilish\",\n",
        "    u\">;\\)\":\"Evil or devilish\",\n",
        "    u\"\\|;‚Äë\\)\":\"Cool\",\n",
        "    u\"\\|‚ÄëO\":\"Bored\",\n",
        "    u\":‚ÄëJ\":\"Tongue-in-cheek\",\n",
        "    u\"#‚Äë\\)\":\"Party all night\",\n",
        "    u\"%‚Äë\\)\":\"Drunk or confused\",\n",
        "    u\"%\\)\":\"Drunk or confused\",\n",
        "    u\":-###..\":\"Being sick\",\n",
        "    u\":###..\":\"Being sick\",\n",
        "    u\"<:‚Äë\\|\":\"Dump\",\n",
        "    u\"\\(>_<\\)\":\"Troubled\",\n",
        "    u\"\\(>_<\\)>\":\"Troubled\",\n",
        "    u\"\\(';'\\)\":\"Baby\",\n",
        "    u\"\\(\\^\\^>``\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
        "    u\"\\(\\^_\\^;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
        "    u\"\\(-_-;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
        "    u\"\\(~_~;\\) \\(„Éª\\.„Éª;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
        "    u\"\\(-_-\\)zzz\":\"Sleeping\",\n",
        "    u\"\\(\\^_-\\)\":\"Wink\",\n",
        "    u\"\\(\\(\\+_\\+\\)\\)\":\"Confused\",\n",
        "    u\"\\(\\+o\\+\\)\":\"Confused\",\n",
        "    u\"\\(o\\|o\\)\":\"Ultraman\",\n",
        "    u\"\\^_\\^\":\"Joyful\",\n",
        "    u\"\\(\\^_\\^\\)/\":\"Joyful\",\n",
        "    u\"\\(\\^O\\^\\)Ôºè\":\"Joyful\",\n",
        "    u\"\\(\\^o\\^\\)Ôºè\":\"Joyful\",\n",
        "    u\"\\(__\\)\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"_\\(\\._\\.\\)_\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"<\\(_ _\\)>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"<m\\(__\\)m>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"m\\(__\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"m\\(_ _\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"\\('_'\\)\":\"Sad or Crying\",\n",
        "    u\"\\(/_;\\)\":\"Sad or Crying\",\n",
        "    u\"\\(T_T\\) \\(;_;\\)\":\"Sad or Crying\",\n",
        "    u\"\\(;_;\":\"Sad of Crying\",\n",
        "    u\"\\(;_:\\)\":\"Sad or Crying\",\n",
        "    u\"\\(;O;\\)\":\"Sad or Crying\",\n",
        "    u\"\\(:_;\\)\":\"Sad or Crying\",\n",
        "    u\"\\(ToT\\)\":\"Sad or Crying\",\n",
        "    u\";_;\":\"Sad or Crying\",\n",
        "    u\";-;\":\"Sad or Crying\",\n",
        "    u\";n;\":\"Sad or Crying\",\n",
        "    u\";;\":\"Sad or Crying\",\n",
        "    u\"Q\\.Q\":\"Sad or Crying\",\n",
        "    u\"T\\.T\":\"Sad or Crying\",\n",
        "    u\"QQ\":\"Sad or Crying\",\n",
        "    u\"Q_Q\":\"Sad or Crying\",\n",
        "    u\"\\(-\\.-\\)\":\"Shame\",\n",
        "    u\"\\(-_-\\)\":\"Shame\",\n",
        "    u\"\\(‰∏Ä‰∏Ä\\)\":\"Shame\",\n",
        "    u\"\\(Ôºõ‰∏Ä_‰∏Ä\\)\":\"Shame\",\n",
        "    u\"\\(=_=\\)\":\"Tired\",\n",
        "    u\"\\(=\\^\\¬∑\\^=\\)\":\"cat\",\n",
        "    u\"\\(=\\^\\¬∑\\¬∑\\^=\\)\":\"cat\",\n",
        "    u\"=_\\^=\t\":\"cat\",\n",
        "    u\"\\(\\.\\.\\)\":\"Looking down\",\n",
        "    u\"\\(\\._\\.\\)\":\"Looking down\",\n",
        "    u\"\\^m\\^\":\"Giggling with hand covering mouth\",\n",
        "    u\"\\(\\„Éª\\„Éª?\":\"Confusion\",\n",
        "    u\"\\(?_?\\)\":\"Confusion\",\n",
        "    u\">\\^_\\^<\":\"Normal Laugh\",\n",
        "    u\"<\\^!\\^>\":\"Normal Laugh\",\n",
        "    u\"\\^/\\^\":\"Normal Laugh\",\n",
        "    u\"\\Ôºà\\*\\^_\\^\\*Ôºâ\" :\"Normal Laugh\",\n",
        "    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(^\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^\\.\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^_\\^\\.\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^_\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^J\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\*\\^\\.\\^\\*\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^‚Äî\\^\\Ôºâ\":\"Normal Laugh\",\n",
        "    u\"\\(#\\^\\.\\^#\\)\":\"Normal Laugh\",\n",
        "    u\"\\Ôºà\\^‚Äî\\^\\Ôºâ\":\"Waving\",\n",
        "    u\"\\(;_;\\)/~~~\":\"Waving\",\n",
        "    u\"\\(\\^\\.\\^\\)/~~~\":\"Waving\",\n",
        "    u\"\\(-_-\\)/~~~ \\($\\¬∑\\¬∑\\)/~~~\":\"Waving\",\n",
        "    u\"\\(T_T\\)/~~~\":\"Waving\",\n",
        "    u\"\\(ToT\\)/~~~\":\"Waving\",\n",
        "    u\"\\(\\*\\^0\\^\\*\\)\":\"Excited\",\n",
        "    u\"\\(\\*_\\*\\)\":\"Amazed\",\n",
        "    u\"\\(\\*_\\*;\":\"Amazed\",\n",
        "    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"Amazed\",\n",
        "    u\"\\(\\*\\^\\^\\)v\":\"Laughing,Cheerful\",\n",
        "    u\"\\(\\^_\\^\\)v\":\"Laughing,Cheerful\",\n",
        "    u\"\\(\\(d[-_-]b\\)\\)\":\"Headphones,Listening to music\",\n",
        "    u'\\(-\"-\\)':\"Worried\",\n",
        "    u\"\\(„Éº„Éº;\\)\":\"Worried\",\n",
        "    u\"\\(\\^0_0\\^\\)\":\"Eyeglasses\",\n",
        "    u\"\\(\\ÔºæÔΩñ\\Ôºæ\\)\":\"Happy\",\n",
        "    u\"\\(\\ÔºæÔΩï\\Ôºæ\\)\":\"Happy\",\n",
        "    u\"\\(\\^\\)o\\(\\^\\)\":\"Happy\",\n",
        "    u\"\\(\\^O\\^\\)\":\"Happy\",\n",
        "    u\"\\(\\^o\\^\\)\":\"Happy\",\n",
        "    u\"\\)\\^o\\^\\(\":\"Happy\",\n",
        "    u\":O o_O\":\"Surprised\",\n",
        "    u\"o_0\":\"Surprised\",\n",
        "    u\"o\\.O\":\"Surpised\",\n",
        "    u\"\\(o\\.o\\)\":\"Surprised\",\n",
        "    u\"oO\":\"Surprised\",\n",
        "    u\"\\(\\*Ôø£mÔø£\\)\":\"Dissatisfied\",\n",
        "    u\"\\(‚ÄòA`\\)\":\"Snubbed or Deflated\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-bVUF6gkqbB"
      },
      "outputs": [],
      "source": [
        "def remove_emoticons(text):\n",
        "    emoticons_pattern = re.compile(u'(' + u'|'.join(emo for emo in EMOTICONS) + u')')\n",
        "    return emoticons_pattern.sub(r'', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guorg3X9kqbC"
      },
      "outputs": [],
      "source": [
        "remove_emoticons(\"Hello :->\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LD8SVcfYkqbC"
      },
      "outputs": [],
      "source": [
        "# Remove emoticons from text\n",
        "df.text = df.text.apply(lambda x: remove_emoticons(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do-EN2y3kqbC"
      },
      "source": [
        "### Hashtags and Mentions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcY2UWpVkqbH"
      },
      "source": [
        "We are habituated to use hashtags and mentions in our tweet either to indicate the context or bring attention to an individual. Hashtags can be used to extract features, to see what's trending and in various other applications.\n",
        "\n",
        "Since, we don't require them we'll remove them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQlGt6uFkqbH"
      },
      "outputs": [],
      "source": [
        "def remove_tags_mentions(text):\n",
        "    pattern = re.compile(r'(@\\S+|#\\S+)')\n",
        "    return pattern.sub('', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xz4u4zhpkqbH"
      },
      "outputs": [],
      "source": [
        "text = \"live @flippinginja on #younow - jonah and jareddddd\"\n",
        "remove_tags_mentions(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aA8vhZURkqbI"
      },
      "outputs": [],
      "source": [
        "# Remove hashtags and mentions\n",
        "df.text = df.text.apply(lambda x: remove_tags_mentions(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKYHM9QbkqbI"
      },
      "source": [
        "### Punctuations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MbDpvoSkqbI"
      },
      "source": [
        "Punctuations are character other than alphaters and digits. These include [!\"#$%&\\'()*+,-./:;<=>?@\\\\^_`{|}~]\n",
        "\n",
        "It is better remove or convert emoticons before removing the punctuations, since if we do the other we around we might loose the emoticons from the text. Another example, if the text contains $10.50 then we'll remove the .(dot) and the value will loose it's meaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3QecoyWkqbI"
      },
      "outputs": [],
      "source": [
        "PUNCTUATIONS = string.punctuation\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', PUNCTUATIONS))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taWiwyT6kqbI"
      },
      "outputs": [],
      "source": [
        "df.text = df[\"text\"].apply(lambda text: remove_punctuation(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xEQXB2QkqbJ"
      },
      "source": [
        "### Stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_TSpSslkqbJ"
      },
      "source": [
        "Stopwords are commonly occuring words in any language. Such as, in english these words are 'the', 'a', 'an', & many more. They are in most cases not useful and should be removed.\n",
        "\n",
        "There are certain tasks in which these words are useful such as Part-of-Speech(POS) tagging, language translation. Stopwords are compiled for many languages, for english language we can use the list from the nltk package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIKpUs5YkqbJ"
      },
      "outputs": [],
      "source": [
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    return ' '.join([word for word in text.split() if word not in STOPWORDS])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TjlrMnvkqbJ"
      },
      "outputs": [],
      "source": [
        "# Remove stopwords\n",
        "df.text = df.text.apply(lambda text: remove_stopwords(text))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLT-zkutkqbJ"
      },
      "source": [
        "### Numbers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXuy621nkqbK"
      },
      "source": [
        "We may remove numbers if they are not useful in our analysis. But analysis in the financial domain, numbers are very useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjrfEo_rkqbK"
      },
      "outputs": [],
      "source": [
        "df.text = df.text.str.replace(r'\\d+', '', regex=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZE9ks9gkqbK"
      },
      "source": [
        "### Extra whitespaces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cvn69G69kqbK"
      },
      "source": [
        "After usually after preprocessing the text there might be extra whitespaces that might be created after transforming, removing various characters. Also, there is a need to remove all the new line, tab characters as well from our text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6aQeSRdkqbL"
      },
      "outputs": [],
      "source": [
        "def remove_whitespaces(text):\n",
        "    return \" \".join(text.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwEMRfG9kqbL"
      },
      "outputs": [],
      "source": [
        "text = \"  Whitespaces in the beginning are removed  \\t as well \\n  as in between  the text   \"\n",
        "\n",
        "clean_text = \" \".join(text.split())\n",
        "clean_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGFhlsE8kqbL"
      },
      "outputs": [],
      "source": [
        "df.text = df.text.apply(lambda x: remove_whitespaces(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpS2X2K2kqbL"
      },
      "source": [
        "### Frequent words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4KdTq2gkqbM"
      },
      "source": [
        "Previously we have removed stopwords which are common in any language. If we are working in any domain, we can also remove the common words used in that domain which don't provide us with much information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSEAM4qCkqbN"
      },
      "outputs": [],
      "source": [
        "def freq_words(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    FrequentWords = []\n",
        "\n",
        "    for word in tokens:\n",
        "        counter[word] += 1\n",
        "\n",
        "    for (word, word_count) in counter.most_common(10):\n",
        "        FrequentWords.append(word)\n",
        "    return FrequentWords\n",
        "\n",
        "def remove_fw(text, FrequentWords):\n",
        "    tokens = word_tokenize(text)\n",
        "    without_fw = []\n",
        "    for word in tokens:\n",
        "        if word not in FrequentWords:\n",
        "            without_fw.append(word)\n",
        "\n",
        "    without_fw = ' '.join(without_fw)\n",
        "    return without_fw\n",
        "\n",
        "counter = Counter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6w-84Ui4kqbO"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "Natural Language Processing is the technology used to aid computers to understand the human‚Äôs natural language. It‚Äôs not an easy task teaching machines to understand how we communicate. Leand Romaf, an experienced software engineer who is passionate at teaching people how artificial intelligence systems work, says that ‚Äúin recent years, there have been significant breakthroughs in empowering computers to understand language just as we do.‚Äù This article will give a simple introduction to Natural Language Processing and how it can be achieved. Natural Language Processing, usually shortened as NLP, is a branch of artificial intelligence that deals with the interaction between computers and humans using the natural language. The ultimate objective of NLP is to read, decipher, understand, and make sense of the human languages in a manner that is valuable. Most NLP techniques rely on machine learning to derive meaning from human languages.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srD1_JJFkqbO"
      },
      "outputs": [],
      "source": [
        "FrequentWords = freq_words(text)\n",
        "print(FrequentWords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOncof3WkqbO"
      },
      "outputs": [],
      "source": [
        "fw_result = remove_fw(text, FrequentWords)\n",
        "fw_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8CEtUTPkqbP"
      },
      "source": [
        "### Rare words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-dHA3vrkqbP"
      },
      "source": [
        "Rare words are similar to frequent words. We can remove them because they are so less that they cannot add any value to the purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RmFp_ETkqbP"
      },
      "outputs": [],
      "source": [
        "def rare_words(text):\n",
        "    # tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "    for word in tokens:\n",
        "        counter[word]= +1\n",
        "\n",
        "    RareWords = []\n",
        "    number_rare_words = 10\n",
        "    # take top 10 frequent words\n",
        "    frequentWords = counter.most_common()\n",
        "    for (word, word_count) in frequentWords[:-number_rare_words:-1]:\n",
        "        RareWords.append(word)\n",
        "\n",
        "    return RareWords\n",
        "\n",
        "def remove_rw(text, RareWords):\n",
        "    tokens = word_tokenize(text)\n",
        "    without_rw = []\n",
        "    for word in tokens:\n",
        "        if word not in RareWords:\n",
        "            without_rw.append(word)\n",
        "\n",
        "    without_rw = ' '.join(without_fw)\n",
        "    return without_rw\n",
        "\n",
        "counter = Counter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYgsR-PDkqbP"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "Natural Language Processing is the technology used to aid computers to understand the human‚Äôs natural language. It‚Äôs not an easy task teaching machines to understand how we communicate. Leand Romaf, an experienced software engineer who is passionate at teaching people how artificial intelligence systems work, says that ‚Äúin recent years, there have been significant breakthroughs in empowering computers to understand language just as we do.‚Äù This article will give a simple introduction to Natural Language Processing and how it can be achieved. Natural Language Processing, usually shortened as NLP, is a branch of artificial intelligence that deals with the interaction between computers and humans using the natural language. The ultimate objective of NLP is to read, decipher, understand, and make sense of the human languages in a manner that is valuable. Most NLP techniques rely on machine learning to derive meaning from human languages.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSgA3ahvkqbP"
      },
      "outputs": [],
      "source": [
        "RareWords = rare_words(text)\n",
        "RareWords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WApiLb7kqbQ"
      },
      "outputs": [],
      "source": [
        "rw_result = remove_fw(text, RareWords)\n",
        "rw_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CodRAZ6okqbQ"
      },
      "source": [
        "## Conversion of Emoji to Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApBGPS_MkqbQ"
      },
      "source": [
        "To remove or not is done based on the purpose of the application. Example if we are building a sentiment analysis system emoji's can be useful.\n",
        "\n",
        "\"The movie was üî•\"\n",
        "or\n",
        "\"The movie was üí©\"\n",
        "\n",
        "If we remove the emoji's the meaning of the sentence changes completely. In these cases we can convert emoji's to words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zvwnaSBkqbR"
      },
      "source": [
        "demoji requires an initial data download from the Unicode Consortium's [emoji code repository](http://unicode.org/Public/emoji/12.0/emoji-test.txt).\n",
        "\n",
        "On first use of the package, call download_codes().<br>\n",
        "This will store the Unicode hex-notated symbols at ~/.demoji/codes.json for future use.\n",
        "\n",
        "Read more about demoji on [pypi.org](https://pypi.org/project/demoji/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Vx--BgWkqbR"
      },
      "outputs": [],
      "source": [
        "demoji.download_codes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYcvXsKtkqbR"
      },
      "outputs": [],
      "source": [
        "def emoji_to_words(text):\n",
        "    return demoji.replace_with_desc(text, sep=\"__\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esIa7enPkqbR"
      },
      "outputs": [],
      "source": [
        "text = \"game is on üî• üö£üèº\"\n",
        "emoji_to_words(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1su8MvMWkqbS"
      },
      "source": [
        "## Conversion of Emoticons to Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asDXFfxJkqbT"
      },
      "source": [
        "As we did for emoji's, we convert emoticons to words for the same purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaIuupb0kqbT"
      },
      "outputs": [],
      "source": [
        "def emoticons_to_words(text):\n",
        "    for emot in EMOTICONS:\n",
        "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").replace(\":\",\"\").split()), text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKBf2yDCkqbT"
      },
      "outputs": [],
      "source": [
        "text = \"Hey there!! :-)\"\n",
        "emoticons_to_words(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiWyURqMkqbT"
      },
      "source": [
        "## Converting Numbers to Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SRNg7CokqbT"
      },
      "source": [
        "If our analysis require us to use information based on the numbers in the text, we can convert them to words.\n",
        "\n",
        "Read more about num2words on [github](https://github.com/savoirfairelinux/num2words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8SLoFqAkqbT"
      },
      "outputs": [],
      "source": [
        "def nums_to_words(text):\n",
        "    new_text = []\n",
        "    for word in text.split():\n",
        "        if word.isdigit():\n",
        "            new_text.append(num2words(word))\n",
        "        else:\n",
        "            new_text.append(word)\n",
        "    return \" \".join(new_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zi4eoMnOkqbU"
      },
      "outputs": [],
      "source": [
        "text = \"I ran this track 30 times\"\n",
        "nums_to_words(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7-HMz2XkqbU"
      },
      "source": [
        "## Chat words Conversion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8wEgDKYkqbU"
      },
      "source": [
        "The more we use social media, we have become lazy to type the whole phrase or word. Due to which slang words came into existance such as \"omg\" which represents \"Oh my god\". Such slang words don't provide much information and if we need to use them we have to convert them.\n",
        "\n",
        "Thank you: [GitHub repo](https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt) for the list of slang words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eh68EhSkqbU"
      },
      "outputs": [],
      "source": [
        "#collapse-hide\n",
        "chat_words = \"\"\"\n",
        "AFAIK=As Far As I Know\n",
        "AFK=Away From Keyboard\n",
        "ASAP=As Soon As Possible\n",
        "ATK=At The Keyboard\n",
        "ATM=At The Moment\n",
        "A3=Anytime, Anywhere, Anyplace\n",
        "BAK=Back At Keyboard\n",
        "BBL=Be Back Later\n",
        "BBS=Be Back Soon\n",
        "BFN=Bye For Now\n",
        "B4N=Bye For Now\n",
        "BRB=Be Right Back\n",
        "BRT=Be Right There\n",
        "BTW=By The Way\n",
        "B4=Before\n",
        "B4N=Bye For Now\n",
        "CU=See You\n",
        "CUL8R=See You Later\n",
        "CYA=See You\n",
        "FAQ=Frequently Asked Questions\n",
        "FC=Fingers Crossed\n",
        "FWIW=For What It's Worth\n",
        "FYI=For Your Information\n",
        "GAL=Get A Life\n",
        "GG=Good Game\n",
        "GN=Good Night\n",
        "GMTA=Great Minds Think Alike\n",
        "GR8=Great!\n",
        "G9=Genius\n",
        "IC=I See\n",
        "ICQ=I Seek you (also a chat program)\n",
        "ILU=ILU: I Love You\n",
        "IMHO=In My Honest/Humble Opinion\n",
        "IMO=In My Opinion\n",
        "IOW=In Other Words\n",
        "IRL=In Real Life\n",
        "KISS=Keep It Simple, Stupid\n",
        "LDR=Long Distance Relationship\n",
        "LMAO=Laugh My A.. Off\n",
        "LOL=Laughing Out Loud\n",
        "LTNS=Long Time No See\n",
        "L8R=Later\n",
        "MTE=My Thoughts Exactly\n",
        "M8=Mate\n",
        "NRN=No Reply Necessary\n",
        "OIC=Oh I See\n",
        "PITA=Pain In The A..\n",
        "PRT=Party\n",
        "PRW=Parents Are Watching\n",
        "QPSA?=Que Pasa?\n",
        "ROFL=Rolling On The Floor Laughing\n",
        "ROFLOL=Rolling On The Floor Laughing Out Loud\n",
        "ROTFLMAO=Rolling On The Floor Laughing My A.. Off\n",
        "SK8=Skate\n",
        "STATS=Your sex and age\n",
        "ASL=Age, Sex, Location\n",
        "THX=Thank You\n",
        "TTFN=Ta-Ta For Now!\n",
        "TTYL=Talk To You Later\n",
        "U=You\n",
        "U2=You Too\n",
        "U4E=Yours For Ever\n",
        "WB=Welcome Back\n",
        "WTF=What The F...\n",
        "WTG=Way To Go!\n",
        "WUF=Where Are You From?\n",
        "W8=Wait...\n",
        "7K=Sick:-D Laugher\n",
        "OMG=Oh my god\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lny7YUeOkqbV"
      },
      "outputs": [],
      "source": [
        "chat_words_dict = dict()\n",
        "chat_words_set = set()\n",
        "\n",
        "def cw_conversion(text):\n",
        "    new_text = []\n",
        "    for word in text.split():\n",
        "        if word.upper() in chat_words_set:\n",
        "            new_text.append(chat_words_dict[word.upper()])\n",
        "        else:\n",
        "            new_text.append(word)\n",
        "    return \" \".join(new_text)\n",
        "\n",
        "for line in chat_words.split('\\n'):\n",
        "    if line != '':\n",
        "        cw, cw_expanded = line.split('=')[0], line.split('=')[1]\n",
        "\n",
        "        chat_words_set.add(cw)\n",
        "        chat_words_dict[cw] = cw_expanded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdXm-qV5kqbV"
      },
      "outputs": [],
      "source": [
        "text = \"omg that's awesome.\"\n",
        "cw_conversion(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAq4dm_DkqbV"
      },
      "source": [
        "## Expanding Contractions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFZ99mg0kqbV"
      },
      "source": [
        "Contractions are words or combinations of words created by dropping a few letters and replacing those letters by an apostrophe.\n",
        "\n",
        "Example:\n",
        "- don't: do not\n",
        "- we'll: we will\n",
        "\n",
        "Our nlp model don't understand these contractions i.e. they don't understand that \"don't\" and \"do not\" are the same thing. If our problem statement requires them then we can expand them or else leave it as it is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hE-ClX_XkqbW"
      },
      "outputs": [],
      "source": [
        "def expand_contractions(text):\n",
        "    expanded_text = []\n",
        "    for line in text:\n",
        "        expanded_text.append(contractions.fix(line))\n",
        "    return expanded_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8IU3ksCkqbW"
      },
      "outputs": [],
      "source": [
        "text = [\"I'll be there within 15 minutes.\", \"It's awesome to meet your new friends.\"]\n",
        "expand_contractions(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SO3ONcKkqbW"
      },
      "source": [
        "## Stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAEEFXLCkqbW"
      },
      "source": [
        "In stemming we reduce the word to it's base or root form by removing the suffix characters from the word. It is one of the technique to normalize text.\n",
        "\n",
        "Stemming for root word \"like\" include:\n",
        "- \"likes\"\n",
        "- \"liked\"\n",
        "- \"likely\"\n",
        "- \"liking\"\n",
        "\n",
        "Stemmed word doesn't always match the words in our dictionary such as:\n",
        "- console -> consol\n",
        "- company -> compani\n",
        "- welcome -> welcom\n",
        "\n",
        "Due to which stemming is not performed in all nlp tasks.\n",
        "\n",
        "There are various algorithms used for stemming but the most widely used is PorterStemmer. In this post we have used the PorterStemmer as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ow4AqiThkqbW"
      },
      "outputs": [],
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stem_words(text):\n",
        "    return ' '.join([stemmer.stem(word) for word in text.split()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q83YCtDpkqbW"
      },
      "outputs": [],
      "source": [
        "df['text_stemmed'] = df.text.apply(lambda text: stem_words(text))\n",
        "df[['text', 'text_stemmed']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnB6qr7RkqbX"
      },
      "source": [
        "PorterStemmer can be used only for english. If we are working with other than english then we can use SnowballStemmer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wE48Xz-DkqbX"
      },
      "outputs": [],
      "source": [
        "SnowballStemmer.languages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4DuNWkIkqbX"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW76K52EkqbX"
      },
      "source": [
        "Lemmatization tried to perform the similar task as that of stemming i.e. trying to reduce the inflection words to it's base form. But lemmatization does it by using a different approach.\n",
        "\n",
        "Lemmatizations takes into consideration of the morphological analysis of the word. It tries to reduce to words to it's dictionary form which is known as lemma."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJpWU2YskqbX"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def text_lemmatize(text):\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Q1QySLbkqbY"
      },
      "outputs": [],
      "source": [
        "df['text_lemmatized'] = df.text.apply(lambda text: text_lemmatize(text))\n",
        "df[['text', 'text_stemmed', 'text_lemmatized']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnzU5de9kqbY"
      },
      "source": [
        "Difference between Stemming and Lemmatization:\n",
        "\n",
        "|Stemming | Lemmatization |\n",
        "| ---- | ---- |\n",
        "| Fast compared to lemmatization | Slow compared to stemming |\n",
        "| Reduces the word to it's base form by removing the suffix | Uses lexical knowledge to get the base form of the word |\n",
        "| Does not always provide meaning or dictionary form of the original word | Resulting words are always meaningful and dictionary words |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6an9c4PkqbY"
      },
      "source": [
        "## Spelling Correction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs8xt5qXkqbY"
      },
      "source": [
        "We as human always make mistake. Normally incorrect spelling in text are know as typos.\n",
        "\n",
        "Since the NLP model doesn't know the difference between a correct and an incorrect word. For the model \"thanks\" and \"thnks\" are two different words. Therefore, spelling correction is an important step to bring the incorrect words in the correct format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3jK_hWJkqbY"
      },
      "outputs": [],
      "source": [
        "spell = SpellChecker()\n",
        "\n",
        "def correct_spelling(text):\n",
        "    correct_text = []\n",
        "    misspelled_words = spell.unknown(text.split())\n",
        "    for word in text.split():\n",
        "        if word in misspelled_words:\n",
        "            correct_text.append(spell.correction(word))\n",
        "        else:\n",
        "            correct_text.append(word)\n",
        "    return \" \".join(correct_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XOBV1gdkqbY"
      },
      "outputs": [],
      "source": [
        "text = \"Hi, hwo are you doin? I'm good thnks for asking\"\n",
        "correct_spelling(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5zjpA9pkqbZ"
      },
      "outputs": [],
      "source": [
        "text = \"hw are you doin? I'm god thnks\"\n",
        "correct_spelling(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVtlLxunkqbZ"
      },
      "source": [
        "## Convert accented characters to ASCII characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfSHC9pVkqbZ"
      },
      "source": [
        "Accent marks (also referred to as diacritics or diacriticals) usually appear above a character when we press the character for a long time. These need to be remove cause the model cannot distinguish between \"d√®√®p\" and \"deep\". It will consider them as two different words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgruutTxkqbZ"
      },
      "outputs": [],
      "source": [
        "def accented_to_ascii(text):\n",
        "    return unidecode.unidecode(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p14V05Vtkqba"
      },
      "outputs": [],
      "source": [
        "text = \"This is an example text with accented characters like d√®√®p l√®arning √°nd c√∂mputer v√≠s√≠√∂n etc.\"\n",
        "accented_to_ascii(text)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "toc-autonumbering": false,
    "toc-showcode": false,
    "toc-showmarkdowntxt": false,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}